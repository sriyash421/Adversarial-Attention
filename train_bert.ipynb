{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Uncomment below for colab'''\n",
    "# ! git clone https://github.com/sriyash421/Adversarial-Attention.git\n",
    "# ! cd Adversarial-Attention && ls && git checkout bert\n",
    "# ! pip install transformers\n",
    "# import sys\n",
    "# sys.path.append('./Adversarial-Attention')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'generate_embeddings'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a4d2bb3178f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgenerate_embeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnn_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEmotionRegression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFeatureExtaction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdamW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'generate_embeddings'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from generate_embeddings import *\n",
    "from nn_utils import Attention, Discriminator, EmotionRegression, FeatureExtaction\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda:0\n"
    }
   ],
   "source": [
    "# torch.manual_seed(0)\n",
    "batch_size = 64\n",
    "lr_attn = 1e-4\n",
    "lr_feature = 8e-5\n",
    "lr_regressor = 4e-5\n",
    "lr_discriminator = 4e-5\n",
    "epochs = 100\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoBank(Dataset) :\n",
    "    def __init__(self, PATH=\"./Adversarial-Attention/data/emobank.csv\", type=None, target_type=['V','A']) :\n",
    "        self.data = pd.read_csv(PATH).to_dict(orient=\"records\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "        self.sentences = []\n",
    "        self.targets = []\n",
    "\n",
    "        for i in trange(len(self.data)) :\n",
    "            item = self.data[i]\n",
    "            if(type != item[\"split\"]) :\n",
    "                continue\n",
    "            sentence = item[\"text\"]\n",
    "            # urls = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', sentence)\n",
    "            # for url in urls: sentence = sentence.replace(url,'')\n",
    "            target = torch.tensor([item[i] for i in target_type])\n",
    "            self.sentences.append(sentence)\n",
    "            self.targets.append(target)\n",
    "\n",
    "        self.encode()\n",
    "        print(\"Dataset {} size: {}\".format(type, len(self.sentences)))\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx) :\n",
    "        return self.input_ids[idx], self.attention_masks[idx], self.token_type_ids[idx], self.targets[idx], self.source_lengths[idx]\n",
    "\n",
    "    def encode(self) :\n",
    "        self.input_ids = []\n",
    "        self.attention_masks = []\n",
    "        self.token_type_ids = []\n",
    "        self.max_len, self.source_lengths = self.max_length()\n",
    "        for sent in self.sentences :\n",
    "            encoded_dict = self.tokenizer.encode_plus(sent,  \n",
    "                                                    max_length=self.max_len, \n",
    "                                                    pad_to_max_length=\"True\", \n",
    "                                                    return_attention_mask = True,\n",
    "                                                    return_tensors = 'pt', \n",
    "                                                    return_token_type_ids = True,\n",
    "                                                    return_lengths = True)\n",
    "            self.input_ids.append(encoded_dict['input_ids'])\n",
    "            self.attention_masks.append(encoded_dict['attention_mask'])\n",
    "            self.token_type_ids.append(encoded_dict['token_type_ids'])\n",
    "        \n",
    "        self.input_ids = torch.cat(self.input_ids, dim=0)\n",
    "        self.attention_masks = torch.cat(self.attention_masks, dim=0)\n",
    "        self.token_type_ids = torch.cat(self.token_type_ids, dim=0)\n",
    "        self.source_lengths = torch.LongTensor(self.source_lengths)\n",
    "        print(\"input ids: {} attention_masks: {} token_type_ids: {} source_lengths: {}\".format(\n",
    "            self.input_ids.shape, self.attention_masks.shape, self.token_type_ids.shape, self.source_lengths.shape))\n",
    "  \n",
    "    def max_length(self) :\n",
    "        max_len = 0\n",
    "        lengths = []\n",
    "        for sent in self.sentences:\n",
    "            input_ids = self.tokenizer.encode(sent)\n",
    "            max_len = max(max_len, len(input_ids))\n",
    "            lengths.append(min(512, len(input_ids)))\n",
    "        max_len = min(512, max_len)\n",
    "        return max_len, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-98652cdf71a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmoBank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdev_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmoBank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmoBank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(EmoBank(type=\"train\"), batch_size=batch_size, shuffle=True)\n",
    "dev_dataloader = DataLoader(EmoBank(type=\"dev\"), batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(EmoBank(type=\"test\"), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAN(nn.Module) :\n",
    "    def __init__(self, embed_size=768, hidden_size=150) :\n",
    "        #['V', 'A', 'D', 'S']\n",
    "        super(AAN, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.feature_size = self.hidden_size*4\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        for name, param in self.bert.named_parameters() :\n",
    "          if \"11\" not in name :\n",
    "              param.requires_grad = False\n",
    "\n",
    "        self.attention_1 = Attention(self.embed_size)\n",
    "        self.attention_2 = Attention(self.embed_size)\n",
    "        self.attention_s = Attention(self.embed_size)\n",
    "\n",
    "        self.features = FeatureExtaction(self.embed_size, self.hidden_size)\n",
    "        \n",
    "        self.regression_1 = EmotionRegression(self.feature_size * 2)\n",
    "        self.regression_2 = EmotionRegression(self.feature_size * 2)\n",
    "\n",
    "        self.discriminator = Discriminator(self.feature_size)\n",
    "    \n",
    "    def forward(self,input_ids, attn_masks, token_type_ids, source_lengths) :\n",
    "        sentences = self.bert(input_ids, attn_masks, token_type_ids)[0]\n",
    "\n",
    "        sentences_1 = self.attention_1(sentences, source_lengths)\n",
    "        sentences_2 = self.attention_2(sentences, source_lengths)\n",
    "        sentences_s = self.attention_s(sentences, source_lengths)\n",
    "\n",
    "        features_1 = self.features(sentences_1, source_lengths)\n",
    "        features_2 = self.features(sentences_2, source_lengths)\n",
    "        features_s = self.features(sentences_s, source_lengths)\n",
    "\n",
    "        value_1 = self.regression_1(torch.cat((features_1, features_s), dim=1))\n",
    "        value_2 = self.regression_2(torch.cat((features_2, features_s), dim=1))\n",
    "\n",
    "        p1, p2 = self.discriminator(features_1), self.discriminator(features_2)\n",
    "        \n",
    "        return value_1, value_2, p1, p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train() :\n",
    "    def __init__(self, type=[0,1]) :\n",
    "        super(Train, self).__init__()\n",
    "        self.type = type\n",
    "        self.model = AAN()\n",
    "        self.model.cuda()\n",
    "        self.init_weights()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bert_optim = AdamW(self.model.bert.parameters(), lr=2e-5)\n",
    "        self.attention_optim = optim.Adam(\n",
    "            list(self.model.attention_1.parameters())+\n",
    "            list(self.model.attention_2.parameters())+\n",
    "            list(self.model.attention_s.parameters()), lr=lr_attn)\n",
    "        self.attention_optim_adversarial = optim.Adam(\n",
    "            list(self.model.attention_1.parameters())+\n",
    "            list(self.model.attention_2.parameters())+\n",
    "            list(self.model.attention_s.parameters()), lr=lr_attn)\n",
    "        self.feature_optim = optim.Adam(self.model.features.parameters(), lr=lr_feature)\n",
    "        self.regressor_optim = optim.RMSprop(\n",
    "            list(self.model.regression_1.parameters())+\n",
    "            list(self.model.regression_1.parameters()), lr=lr_regressor)\n",
    "        self.discriminator_optim = optim.RMSprop(self.model.discriminator.parameters(), lr=lr_discriminator)\n",
    "        self.training_stats = []\n",
    "    \n",
    "    def init_weights(self) :\n",
    "        for name, param in self.model.named_parameters() :\n",
    "            if 'bert' in name :\n",
    "                continue\n",
    "            temp = np.sqrt(6.0/(sum([i for i in param.shape])+1e-8))\n",
    "            param.data.uniform_(-temp, temp)\n",
    "    \n",
    "    def run_model(self, batch) :\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        token_type_ids = batch[2].to(device)\n",
    "        source_lengths = batch[4].to(device)\n",
    "        value_1, value_2, p1, p2 = self.model(input_ids, attention_masks, token_type_ids, source_lengths)\n",
    "        output = torch.cat((value_1, value_2), dim=1)\n",
    "        return output, p1, p2\n",
    "    \n",
    "    def get_r(self, output, target) :\n",
    "        temp = [ stats.pearsonr(output[:,i].cpu().detach(), target[:,i].cpu().detach())[0] for i in range(2)]\n",
    "        return temp\n",
    "    \n",
    "    def train(self, epochs=epochs) :\n",
    "        mse = nn.MSELoss()\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(self.bert_optim, \n",
    "                                            num_warmup_steps = 0,\n",
    "                                            num_training_steps = total_steps)\n",
    "        for epoch_i in range(epochs) :\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            self.model.train()\n",
    "            train_r = []\n",
    "            train_loss = []\n",
    "            for i, batch in enumerate(train_dataloader) :\n",
    "\n",
    "                self.attention_optim.zero_grad()\n",
    "                self.feature_optim.zero_grad()\n",
    "                self.regressor_optim.zero_grad()\n",
    "                self.bert_optim.zero_grad()\n",
    "\n",
    "                target = batch[3].to(device)\n",
    "                output, p1, p2 = self.run_model(batch)\n",
    "                reg_loss = mse(output, target)\n",
    "                train_loss.append(reg_loss.item())\n",
    "                reg_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.bert.parameters(), 1.0)\n",
    "\n",
    "                self.attention_optim.step()\n",
    "                self.feature_optim.step()\n",
    "                self.regressor_optim.step()\n",
    "                self.bert_optim.step()\n",
    "\n",
    "                self.discriminator_optim.zero_grad()\n",
    "\n",
    "                output, p1, p2 = self.run_model(batch)\n",
    "                wloss = (p2-p1).mean()\n",
    "                wloss.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                \n",
    "                self.discriminator_optim.step()\n",
    "\n",
    "                self.attention_optim_adversarial.zero_grad()\n",
    "                \n",
    "                output, p1, p2 = self.run_model(batch)\n",
    "                adversarial_loss = (p1-p2).mean()\n",
    "                adversarial_loss.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                \n",
    "                self.attention_optim_adversarial.step()\n",
    "\n",
    "                train_r.append(self.get_r(output, target))\n",
    "                if i%10 == 0 :\n",
    "                    print(\"Batch: {} train Loss: {} train_r: {}\".format(i, reg_loss, train_r[-1]))\n",
    "\n",
    "            val_loss = []\n",
    "            val_r = []\n",
    "            for i, batch in enumerate(dev_dataloader) :\n",
    "                with torch.no_grad() :\n",
    "                    target = batch[3].to(device)\n",
    "                    output, p1, p2 = self.run_model(batch)\n",
    "\n",
    "                    reg_loss = mse(output, target)\n",
    "                    val_loss.append(reg_loss.item())\n",
    "                    val_r.append(self.get_r(output, target))\n",
    "                    if i%10 == 0 :\n",
    "                        print(\"Batch: {} val Loss: {} val_r: {}\".format(i, reg_loss, val_r[-1]))\n",
    "\n",
    "            self.training_stats.append({\n",
    "                'training loss' : sum(train_loss)/len(train_loss),\n",
    "                'validation loss' : sum(val_loss)/len(val_loss),\n",
    "                'train r' : torch.tensor(train_r).mean(dim=0),\n",
    "                'val r' : torch.tensor(val_r).mean(dim=0),\n",
    "            })\n",
    "            print(self.training_stats[-1])\n",
    "            \n",
    "        def save_model(self) :\n",
    "            #TODO: Add function to save model\n",
    "            pass\n",
    "        \n",
    "        def plot(self, r_values) :\n",
    "            #TODO: Plot r values\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e0940563d568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train = Train()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train = Train()\n",
    "train.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = {\n",
    "        \"attention_1\":train.model.attention_1.state_dict(),\n",
    "        \"attention_2\":train.model.attention_2.state_dict(),\n",
    "        \"attention_s\":train.model.attention_s.state_dict(),\n",
    "        \"features\":train.model.features.state_dict(),   \n",
    "        \"regression_1\":train.model.regression_1.state_dict(),\n",
    "        \"regression_2\":train.model.regression_1.state_dict(),\n",
    "        \"discriminator\":train.model.discriminator.state_dict()\n",
    "        }\n",
    "torch.save(model_state_dict, \"VA_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Batch: 0 val Loss: 0.07396862655878067 val_r: [0.47933176870423644, 0.3004015398340143]\n"
    }
   ],
   "source": [
    "val_loss = []\n",
    "val_r = []\n",
    "mse = nn.MSELoss()\n",
    "training_stats = []\n",
    "for i, batch in enumerate(test_dataloader) :\n",
    "    with torch.no_grad() :\n",
    "        target = batch[3].to(device)\n",
    "        output, p1, p2 = train.run_model(batch)\n",
    "\n",
    "        reg_loss = mse(output, target)\n",
    "        val_loss.append(reg_loss.item())\n",
    "        val_r.append(train.get_r(output, target))\n",
    "        if i%10 == 0 :\n",
    "            print(\"Batch: {} val Loss: {} val_r: {}\".format(i, reg_loss, val_r[-1]))\n",
    "\n",
    "# training_stats.append({\n",
    "#     'validation loss' : sum(val_loss)/len(val_loss),\n",
    "#     'val r' : torch.tensor(val_r).mean(dim=0),\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(value1, value2, label, index=None) :\n",
    "    sns.set(style='darkgrid')\n",
    "    sns.set(font_scale=1.5)\n",
    "    # plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    if index is not None: \n",
    "        value1 = [i[index] for i in value1]\n",
    "        value2 = [i[index] for i in value2]\n",
    "    plt.plot(value1, 'b-o', label=\"Training\")\n",
    "    plt.plot(value2, 'g-o', label=\"Validation\")\n",
    "\n",
    "    plt.title(\"Training & Validation \"+format(label))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(label)\n",
    "    plt.legend()\n",
    "    # plt.xticks(len(value1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9fbbd1aad775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train r\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val r\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Valence R Value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df_stats = pd.DataFrame(data=train.training_stats)\n",
    "plot(df_stats[\"training loss\"].values[10:],df_stats[\"validation loss\"].values[10:],\"Loss\")\n",
    "plot(df_stats[\"train r\"],df_stats[\"val r\"],\"Valence R Value\", index=0)\n",
    "plot(df_stats[\"train r\"],df_stats[\"val r\"],\"Dominance R Value\", index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('humanoid': conda)",
   "language": "python",
   "name": "python37764bithumanoidcondac78ab0d25f4c4a4593af82371244ab3e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
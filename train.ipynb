{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from nn_utils import Attention, Discriminator, EmotionRegression, FeatureExtaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "lr_attn = 1e-4\n",
    "lr_feature = 8e-5\n",
    "lr_regressor = 4e-5\n",
    "lr_discriminator = 4e-5\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.pkl'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c12f3bcdbf49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.pkl'"
     ]
    }
   ],
   "source": [
    "dict = {}\n",
    "with open(\"dataset.pkl\", \"rb\") as fin :\n",
    "    dict = pickle.load(fin)\n",
    "\n",
    "vocab = dict['vocab']\n",
    "train_dataloader = DataLoader(\n",
    "    dict['train_dataset'],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    "    )\n",
    "val_dataloader = DataLoader(\n",
    "    dict['val_dataset'],\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAN(nn.Module) :\n",
    "    def __init__(self, embed_size=300, hidden_size=150) :\n",
    "        #['V', 'A', 'D', 'S']\n",
    "        super(AAN, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.feature_size = self.hidden_size*4\n",
    "\n",
    "        self.attention_1 = Attention(self.embed_size)\n",
    "        self.attention_2 = Attention(self.embed_size)\n",
    "        self.attention_s = Attention(self.embed_size)\n",
    "\n",
    "        self.features = FeatureExtaction(self.embed_size, self.hidden_size)\n",
    "        \n",
    "        self.regression_1 = EmotionRegression(self.feature_size * 2)\n",
    "        self.regression_2 = EmotionRegression(self.feature_size * 2)\n",
    "\n",
    "        self.discriminator = Discriminator(self.features_size)\n",
    "    \n",
    "    def forward(self, sentences, source_lengths) :\n",
    "        sentences = vocab(sentences)\n",
    "\n",
    "        sentences_1 = self.attention_1(sentences, source_lengths)\n",
    "        sentences_2 = self.attention_2(sentences, source_lengths)\n",
    "        sentences_s = self.attention_s(sentences, source_lengths)\n",
    "\n",
    "        features_1 = self.features(sentences_1, source_lengths)\n",
    "        features_2 = self.features(sentences_2, source_lengths)\n",
    "        features_s = self.features(sentences_s, source_lengths)\n",
    "\n",
    "        value_1 = self.regression_1(torch.cat((features_1, features), dim=1))\n",
    "        value_2 = self.regression_2(torch.cat((features_2, features), dim=1))\n",
    "        \n",
    "        p1, p2 = self.discriminator(features_1), self.discriminator(features_2)\n",
    "\n",
    "        return regressor_values, p1, p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "optimizer can only optimize Tensors, but one of the params is Module.parameters",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2c25466c587d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mattention_optim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_attn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfeature_optim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mregressor_optim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSProp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_regressor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/humanoid/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     40\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     41\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/humanoid/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/humanoid/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36madd_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 raise TypeError(\"optimizer can only optimize Tensors, \"\n\u001b[0;32m--> 200\u001b[0;31m                                 \"but one of the params is \" + torch.typename(param))\n\u001b[0m\u001b[1;32m    201\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can't optimize a non-leaf Tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: optimizer can only optimize Tensors, but one of the params is Module.parameters"
     ]
    }
   ],
   "source": [
    "class Train() :\n",
    "    def __init__(self, type=[0,1]) :\n",
    "        self.type = type\n",
    "        self.model = AAN()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.attention_optim = optim.Adam(\n",
    "            list(self.model.attention_1.parameters())+\n",
    "            list(self.model.attention_2.parameters())+\n",
    "            list(self.model.attention_s.parameters()), lr=lr_attn)\n",
    "        self.feature_optim = optim.Adam(self.model.features.parameters(), lr=lr_feature)\n",
    "        self.regressor_optim = optim.RMSProp(\n",
    "            list(self.model.regression_1.paramters())+\n",
    "            list(self.model.regression_1.paramters()), lr=lr_regressor)\n",
    "        self.discriminator_optim = optim.RMSProp(self.model.discriminator.parameters(), lr=lr_discriminator)\n",
    "        self.training_stats = {}\n",
    "    \n",
    "    def run_model(batch) :\n",
    "        sentences = batch[0].to(device)\n",
    "        source_lengths = batch[1].to(device)\n",
    "        target = batch[2].to(device)\n",
    "        return self.model(vocab, sentences, source_lengths), target[:][self.type]\n",
    "    \n",
    "    def get_r(self, output, target) :\n",
    "        return [(stats.pearsonr(output[:][i], target[:][i])) for i in range(2)]\n",
    "    \n",
    "    def train(self, epochs=epochs) :\n",
    "        for epoch in range(epochs) :\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            self.model.train()\n",
    "            train_r = []\n",
    "            train_loss = []\n",
    "            for i, batch in enumerate(train_dataloader) :\n",
    "\n",
    "                self.attention_optim.zero_grad()\n",
    "                self.feature_optim.zero_grad()\n",
    "                self.regressor_optim.zero_grad()\n",
    "\n",
    "                output, p1, p2, target = run_model(model, batch)\n",
    "\n",
    "                reg_loss = mse(output, target)\n",
    "                loss.append(reg_loss)\n",
    "                reg_loss.backward()\n",
    "                self.attention_optim.step()\n",
    "                self.feature_optim.step()\n",
    "                self.regressor_optim.step()\n",
    "\n",
    "                self.discriminator_optim.zero_grad()\n",
    "\n",
    "                wloss = (p2-p1).mean()\n",
    "                wloss.backward()\n",
    "                self.discriminator.step()\n",
    "\n",
    "                self.attention_optim.zero_grad()\n",
    "\n",
    "                adversarial_loss = (p1 - p2).mean()\n",
    "                adversarial_loss.backward()\n",
    "                self.attention_optim.step()\n",
    "\n",
    "                train_r = train_r.append(self.get_r(output, target))\n",
    "\n",
    "                if i%50 == 0 :\n",
    "                    print(\"Batch: {} train Loss: {} train R: {}\".format(i, reg_loss, train_r[-1]))\n",
    "\n",
    "            val_loss = []\n",
    "            val_r = []\n",
    "            for i, batch in enumerate(val_dataloader) :\n",
    "                with torch.no_grad() :\n",
    "                    output, p1, p2, target = run_model(model, batch)\n",
    "\n",
    "                    reg_loss = mse(output, target)\n",
    "                    val_loss.append(reg_loss)\n",
    "                    val_r = val_r.append(self.get_r(output, target))\n",
    "                    if i%50 == 0 :\n",
    "                        print(\"Batch: {} val Loss: {} val R: {}\".format(i, reg_loss, val_r[-1]))\n",
    "\n",
    "            self.training_stats.append({\n",
    "                'training loss' : sum(train_loss)/len(train_loss),\n",
    "                'validation loss' : sum(val_loss)/len(val_loss),\n",
    "                'train r' : toch.tensor(train_r).mean(dim=0),\n",
    "                'val r' : toch.tensor(val_r).mean(dim=0),\n",
    "            })\n",
    "            print(json.dumps(self.training_stats[-1], indent=4))\n",
    "            \n",
    "        def save_model(self) :\n",
    "            #TODO: Add function to save model\n",
    "            pass\n",
    "        \n",
    "        def plot(self, r_values) :\n",
    "            #TODO: Plot r values\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bithumanoidcondac78ab0d25f4c4a4593af82371244ab3e",
   "display_name": "Python 3.7.7 64-bit ('humanoid': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from generate_embeddings import *\n",
    "from nn_utils import Attention, Discriminator, EmotionRegression, FeatureExtaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cuda:0\n"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "lr_attn = 1e-4\n",
    "lr_feature = 8e-5\n",
    "lr_regressor = 4e-5\n",
    "lr_discriminator = 4e-5\n",
    "epochs = 100\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Vocab: torch.Size([17339, 300])\n100%|██████████| 8061/8061 [00:28<00:00, 283.45it/s]\n  4%|▍         | 43/1000 [00:00<00:02, 428.42it/s]Tokens: torch.Size([8061, 109]) Lengths: torch.Size([8061]) Target: torch.Size([8061, 3])\n100%|██████████| 1000/1000 [00:02<00:00, 347.67it/s]\n  3%|▎         | 28/1000 [00:00<00:03, 275.36it/s]Tokens: torch.Size([1000, 73]) Lengths: torch.Size([1000]) Target: torch.Size([1000, 3])\n100%|██████████| 1000/1000 [00:03<00:00, 257.21it/s]Tokens: torch.Size([1000, 134]) Lengths: torch.Size([1000]) Target: torch.Size([1000, 3])\n\n"
    }
   ],
   "source": [
    "dataset = Dataset()\n",
    "vocab = dataset.get_vocab().to(device)\n",
    "train_dataset = dataset.read_data('./data/train.csv') \n",
    "dev_dataset = dataset.read_data('./data/dev.csv')\n",
    "test_dataset = dataset.read_data('./data/test.csv')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    "    )\n",
    "val_dataloader = DataLoader(\n",
    "    dev_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size\n",
    "    )\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAN(nn.Module) :\n",
    "    def __init__(self, embed_size=300, hidden_size=150) :\n",
    "        #['V', 'A', 'D', 'S']\n",
    "        super(AAN, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.feature_size = self.hidden_size*4\n",
    "\n",
    "        self.attention_1 = Attention(self.embed_size)\n",
    "        self.attention_2 = Attention(self.embed_size)\n",
    "        self.attention_s = Attention(self.embed_size)\n",
    "\n",
    "        self.features = FeatureExtaction(self.embed_size, self.hidden_size)\n",
    "        \n",
    "        self.regression_1 = EmotionRegression(self.feature_size * 2)\n",
    "        self.regression_2 = EmotionRegression(self.feature_size * 2)\n",
    "\n",
    "        self.discriminator = Discriminator(self.feature_size)\n",
    "    \n",
    "    def forward(self, vocab, sentences, source_lengths) :\n",
    "        sentences = vocab(sentences)\n",
    "        sentences = sentences.detach()\n",
    "\n",
    "        sentences_1 = self.attention_1(sentences, source_lengths)\n",
    "        sentences_2 = self.attention_2(sentences, source_lengths)\n",
    "        sentences_s = self.attention_s(sentences, source_lengths)\n",
    "\n",
    "        features_1 = self.features(sentences_1, source_lengths)\n",
    "        features_2 = self.features(sentences_2, source_lengths)\n",
    "        features_s = self.features(sentences_s, source_lengths)\n",
    "\n",
    "        value_1 = self.regression_1(torch.cat((features_1, features_s), dim=1))\n",
    "        value_2 = self.regression_2(torch.cat((features_2, features_s), dim=1))\n",
    "\n",
    "        p1, p2 = self.discriminator(features_1), self.discriminator(features_2)\n",
    "        \n",
    "        return value_1, value_2, p1, p2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train() :\n",
    "    def __init__(self, type=[0,1]) :\n",
    "        super(Train, self).__init__()\n",
    "        self.type = type\n",
    "        self.model = AAN().to(device)\n",
    "        # self.init_weights()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.attention_optim = optim.Adam(\n",
    "            list(self.model.attention_1.parameters())+\n",
    "            list(self.model.attention_2.parameters())+\n",
    "            list(self.model.attention_s.parameters()), lr=lr_attn)\n",
    "        self.attention_optim_adversarial = optim.Adam(\n",
    "            list(self.model.attention_1.parameters())+\n",
    "            list(self.model.attention_2.parameters())+\n",
    "            list(self.model.attention_s.parameters()), lr=lr_attn)\n",
    "        self.feature_optim = optim.Adam(self.model.features.parameters(), lr=lr_feature)\n",
    "        self.regressor_optim = optim.RMSprop(\n",
    "            list(self.model.regression_1.parameters())+\n",
    "            list(self.model.regression_1.parameters()), lr=lr_regressor)\n",
    "        self.discriminator_optim = optim.RMSprop(self.model.discriminator.parameters(), lr=lr_discriminator)\n",
    "        self.training_stats = []\n",
    "    \n",
    "    def init_weights(self) :\n",
    "        for param in self.model.parameters() :\n",
    "            temp = np.sqrt(6.0/(sum([i for i in param.shape])+1e-8))\n",
    "            param.data.uniform_(-temp, temp)\n",
    "    \n",
    "    def run_model(self, batch) :\n",
    "        sentences = batch[0].to(device)\n",
    "        source_lengths = batch[1].to(device)\n",
    "        target = batch[2].to(device)\n",
    "        value_1, value_2, p1, p2 = self.model(vocab, sentences, source_lengths)\n",
    "        output = torch.cat((value_1, value_2), dim=1)\n",
    "        target = target[:,self.type].float()\n",
    "        return output, p1, p2, target\n",
    "    \n",
    "    def get_r(self, output, target) :\n",
    "        temp = [ stats.pearsonr(output[:,i].cpu().detach(), target[:,i].cpu().detach())[0] for i in range(2)]\n",
    "        return temp\n",
    "    \n",
    "    def train(self, epochs=epochs) :\n",
    "        mse = nn.MSELoss()\n",
    "        for epoch_i in range(epochs) :\n",
    "            print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "            self.model.train()\n",
    "            train_r = []\n",
    "            train_loss = []\n",
    "            for i, batch in enumerate(train_dataloader) :\n",
    "\n",
    "                self.attention_optim.zero_grad()\n",
    "                self.feature_optim.zero_grad()\n",
    "                self.regressor_optim.zero_grad()\n",
    "\n",
    "                output, p1, p2, target = self.run_model(batch)\n",
    "                reg_loss = mse(output, target)\n",
    "                train_loss.append(reg_loss.item())\n",
    "                reg_loss.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "                self.attention_optim.step()\n",
    "                self.feature_optim.step()\n",
    "                self.regressor_optim.step()\n",
    "\n",
    "                self.discriminator_optim.zero_grad()\n",
    "\n",
    "                output, p1, p2, target = self.run_model(batch)\n",
    "                wloss = (p2-p1).mean()\n",
    "                wloss.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                \n",
    "                self.discriminator_optim.step()\n",
    "\n",
    "                self.attention_optim_adversarial.zero_grad()\n",
    "                \n",
    "                output, p1, p2, target = self.run_model(batch)\n",
    "                adversarial_loss = (p1-p2).mean()\n",
    "                adversarial_loss.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                \n",
    "                self.attention_optim_adversarial.step()\n",
    "\n",
    "                train_r.append(self.get_r(output, target))\n",
    "                if i%10 == 0 :\n",
    "                    print(\"Batch: {} train Loss: {} train_r: {}\".format(i, reg_loss, train_r[-1]))\n",
    "\n",
    "            val_loss = []\n",
    "            val_r = []\n",
    "            for i, batch in enumerate(val_dataloader) :\n",
    "                with torch.no_grad() :\n",
    "                    output, p1, p2, target = self.run_model(batch)\n",
    "\n",
    "                    reg_loss = mse(output, target)\n",
    "                    val_loss.append(reg_loss.item())\n",
    "                    val_r.append(self.get_r(output, target))\n",
    "                    if i%10 == 0 :\n",
    "                        print(\"Batch: {} val Loss: {} val_r: {}\".format(i, reg_loss, val_r[-1]))\n",
    "\n",
    "            self.training_stats.append({\n",
    "                'training loss' : sum(train_loss)/len(train_loss),\n",
    "                'validation loss' : sum(val_loss)/len(val_loss),\n",
    "                'train r' : torch.tensor(train_r).mean(dim=0).item(),\n",
    "                'val r' : torch.tensor(val_r).mean(dim=0).item(),\n",
    "            })\n",
    "            print(json.dumps(self.training_stats[-1], ident=4))\n",
    "            \n",
    "        def save_model(self) :\n",
    "            #TODO: Add function to save model\n",
    "            pass\n",
    "        \n",
    "        def plot(self, r_values) :\n",
    "            #TODO: Plot r values\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "======== Epoch 1 / 10 ========\nBatch: 0 train Loss: 0.07828138768672943 train_r: [0.5775601206275207, 0.3465344150288597]\nBatch: 10 train Loss: 0.06599719077348709 train_r: [0.46358762226571854, 0.19410596117246268]\nBatch: 20 train Loss: 0.06102125346660614 train_r: [0.5300792457119532, 0.2608446691863796]\nBatch: 30 train Loss: 0.05710688233375549 train_r: [0.38688727529512734, 0.23934299925966485]\nBatch: 40 train Loss: 0.06419407576322556 train_r: [0.536086566328229, 0.2156681558311598]\nBatch: 50 train Loss: 0.08376166224479675 train_r: [0.42012290905567484, 0.2908518252889128]\nBatch: 60 train Loss: 0.09530360251665115 train_r: [0.43838315659571647, 0.23874884288153073]\nBatch: 70 train Loss: 0.10118158906698227 train_r: [0.47525394870867854, 0.49035453684012703]\nBatch: 80 train Loss: 0.11314847320318222 train_r: [0.43082864943182975, 0.39633179916840566]\nBatch: 90 train Loss: 0.0939970463514328 train_r: [0.6229797964651029, 0.47433017897496055]\nBatch: 100 train Loss: 0.06778638064861298 train_r: [0.6530572937852828, 0.43663790649767575]\nBatch: 110 train Loss: 0.09758122265338898 train_r: [0.24765941380873358, 0.44555199160191045]\nBatch: 120 train Loss: 0.06725345551967621 train_r: [0.48977043121913455, 0.19158355580785902]\nBatch: 0 val Loss: 0.11699749529361725 val_r: [0.1885057917295278, 0.21937783962667812]\nBatch: 10 val Loss: 0.07230786979198456 val_r: [0.16955974943662389, -0.016948006755670687]\n{'training loss': 0.08196119825163531, 'validation loss': 0.08480235189199448, 'train r': tensor([0.4521, 0.2868]), 'val r': tensor([0.3265, 0.1825])}\n======== Epoch 2 / 10 ========\nBatch: 0 train Loss: 0.08225539326667786 train_r: [0.4035018332681184, 0.10551674377747311]\nBatch: 10 train Loss: 0.1231958344578743 train_r: [0.22339809083669337, 0.4096083411990881]\nBatch: 20 train Loss: 0.07888521999120712 train_r: [0.5311136734159455, 0.07860634972371335]\nBatch: 30 train Loss: 0.060998618602752686 train_r: [0.3710720415062269, 0.5152418250537536]\nBatch: 40 train Loss: 0.07403656095266342 train_r: [0.454440602155784, 0.27221480757353134]\nBatch: 50 train Loss: 0.06040557473897934 train_r: [0.41389237350754227, 0.28274382962216354]\nBatch: 60 train Loss: 0.06580103933811188 train_r: [0.5209556001361384, 0.41056040266085664]\nBatch: 70 train Loss: 0.09220980107784271 train_r: [0.44585269375706627, 0.15725448114849355]\nBatch: 80 train Loss: 0.06969746947288513 train_r: [0.5169726747632377, 0.505671090705255]\nBatch: 90 train Loss: 0.08722534775733948 train_r: [0.48235876787856613, 0.3301983028501795]\nBatch: 100 train Loss: 0.08393992483615875 train_r: [0.18692681277180948, 0.4306191511602758]\nBatch: 110 train Loss: 0.08236047625541687 train_r: [0.47037619806488473, 0.057193633044589826]\nBatch: 120 train Loss: 0.07830876111984253 train_r: [0.5644124940459494, 0.17601181845923244]\nBatch: 0 val Loss: 0.11045677959918976 val_r: [0.2404621440174726, 0.226951177070185]\nBatch: 10 val Loss: 0.0630350187420845 val_r: [0.2211988694887868, -0.004878417213909605]\n{'training loss': 0.0810599931116615, 'validation loss': 0.08104057353921235, 'train r': tensor([0.4620, 0.3010]), 'val r': tensor([0.3589, 0.1929])}\n======== Epoch 3 / 10 ========\nBatch: 0 train Loss: 0.04678020626306534 train_r: [0.5881547325463949, 0.23842422753855558]\nBatch: 10 train Loss: 0.07131379097700119 train_r: [0.6539941431080524, 0.23810286265624517]\nBatch: 20 train Loss: 0.085588239133358 train_r: [0.4400989248302252, 0.35828931186799223]\nBatch: 30 train Loss: 0.08222722262144089 train_r: [0.5269878939682605, 0.2936114685765586]\nBatch: 40 train Loss: 0.06748124957084656 train_r: [0.37247944134526667, 0.37947925685702033]\nBatch: 50 train Loss: 0.11217497289180756 train_r: [0.3165633075189499, 0.3767258549561634]\nBatch: 60 train Loss: 0.09367066621780396 train_r: [0.404533878473895, 0.12862028900211217]\nBatch: 70 train Loss: 0.135047048330307 train_r: [0.48836744880931443, 0.5023520174920293]\nBatch: 80 train Loss: 0.07472706586122513 train_r: [0.4518809187468848, 0.28614558562675646]\nBatch: 90 train Loss: 0.07819448411464691 train_r: [0.3810584235239858, 0.02046292026552582]\nBatch: 100 train Loss: 0.051652662456035614 train_r: [0.5812425243303415, 0.20654892287850313]\nBatch: 110 train Loss: 0.07629047334194183 train_r: [0.3023020218295648, 0.222866158131541]\nBatch: 120 train Loss: 0.06946834921836853 train_r: [0.4772089564140411, 0.6057705170908725]\nBatch: 0 val Loss: 0.11042094230651855 val_r: [0.2614422691289005, 0.23156008578313725]\nBatch: 10 val Loss: 0.06477905809879303 val_r: [0.18189624956313222, 0.006221242983301456]\n{'training loss': 0.0799705417797206, 'validation loss': 0.0815520491451025, 'train r': tensor([0.4807, 0.2957]), 'val r': tensor([0.3479, 0.2049])}\n======== Epoch 4 / 10 ========\nBatch: 0 train Loss: 0.07262028753757477 train_r: [0.3878021020886696, 0.40896177335059425]\nBatch: 10 train Loss: 0.11798850446939468 train_r: [0.33000432403360735, 0.3582385600767278]\nBatch: 20 train Loss: 0.093124158680439 train_r: [0.5712443772986548, 0.30543369942859633]\nBatch: 30 train Loss: 0.07748940587043762 train_r: [0.524468584915251, 0.09890572265003994]\nBatch: 40 train Loss: 0.06617467850446701 train_r: [0.5192016342755348, 0.27713205381873507]\nBatch: 50 train Loss: 0.1223035454750061 train_r: [0.4000118964243773, 0.2771293513646567]\nBatch: 60 train Loss: 0.09540944546461105 train_r: [0.44192544271161854, 0.11754158277769208]\nBatch: 70 train Loss: 0.0814283937215805 train_r: [0.42967844358450524, 0.5755203148476119]\nBatch: 80 train Loss: 0.10822196304798126 train_r: [0.4295158007480126, 0.37182874693795465]\nBatch: 90 train Loss: 0.05442113056778908 train_r: [0.6307200139624867, -0.0005815559796317343]\nBatch: 100 train Loss: 0.06961654126644135 train_r: [0.5798131191177032, 0.4113112829944948]\nBatch: 110 train Loss: 0.08489416539669037 train_r: [0.5890346220011657, 0.4249871197586962]\nBatch: 120 train Loss: 0.07420579344034195 train_r: [0.35492175305461565, 0.1231043298821844]\nBatch: 0 val Loss: 0.10952284187078476 val_r: [0.271434159535604, 0.2270216490404851]\nBatch: 10 val Loss: 0.06433586776256561 val_r: [0.20790181355209558, 0.002985813801957792]\n{'training loss': 0.07946978955869637, 'validation loss': 0.08075313945300877, 'train r': tensor([0.4832, 0.3001]), 'val r': tensor([0.3723, 0.2008])}\n======== Epoch 5 / 10 ========\nBatch: 0 train Loss: 0.07702823728322983 train_r: [0.44543659251370904, 0.47665859125555404]\nBatch: 10 train Loss: 0.08361928164958954 train_r: [0.6088984180323371, 0.534524813785167]\nBatch: 20 train Loss: 0.10357260704040527 train_r: [0.4882332364329801, 0.25576435654132684]\nBatch: 30 train Loss: 0.10115325450897217 train_r: [0.45424628017207624, 0.4653129111479989]\nBatch: 40 train Loss: 0.07989668846130371 train_r: [0.3559642918911495, 0.25830087272291435]\nBatch: 50 train Loss: 0.0807155966758728 train_r: [0.45076828286865916, 0.42538057661352613]\nBatch: 60 train Loss: 0.06193508207798004 train_r: [0.49459927666470266, 0.26598034669467246]\nBatch: 70 train Loss: 0.07720521092414856 train_r: [0.47768918633091556, 0.1926201875085932]\nBatch: 80 train Loss: 0.07754353433847427 train_r: [0.49133472615164964, -0.12523437956995617]\nBatch: 90 train Loss: 0.08266450464725494 train_r: [0.4323732051953437, 0.43936812334222847]\nBatch: 100 train Loss: 0.07137646526098251 train_r: [0.36095856439302065, 0.4525316786128344]\nBatch: 110 train Loss: 0.0907052755355835 train_r: [0.46607614740582354, 0.2855252108618202]\nBatch: 120 train Loss: 0.0781773030757904 train_r: [0.47184030083603656, 0.20273825015966598]\nBatch: 0 val Loss: 0.11248954385519028 val_r: [0.22030137955990053, 0.24125866275934907]\nBatch: 10 val Loss: 0.06455536931753159 val_r: [0.12753680396137548, 0.00778355521913713]\n{'training loss': 0.08081352432805394, 'validation loss': 0.08125635702162981, 'train r': tensor([0.4711, 0.2998]), 'val r': tensor([0.3695, 0.2202])}\n======== Epoch 6 / 10 ========\nBatch: 0 train Loss: 0.0875961184501648 train_r: [0.44373336288613185, 0.3923332919684094]\nBatch: 10 train Loss: 0.07050001621246338 train_r: [0.47179157456394827, -0.04235943175849485]\nBatch: 20 train Loss: 0.0861901044845581 train_r: [0.5123913816142459, 0.5634241179564814]\nBatch: 30 train Loss: 0.06125713512301445 train_r: [0.6543839845446127, 0.04807172453569403]\nBatch: 40 train Loss: 0.06864175945520401 train_r: [0.44336012173128025, 0.6048182081640323]\nBatch: 50 train Loss: 0.07543326169252396 train_r: [0.46942198860868956, 0.38539541833097324]\nBatch: 60 train Loss: 0.08185423165559769 train_r: [0.3471981272578501, 0.5771393839044635]\nBatch: 70 train Loss: 0.0730384960770607 train_r: [0.266754123390324, 0.5052778356664123]\nBatch: 80 train Loss: 0.10432231426239014 train_r: [0.5220950351286937, 0.38088725520654054]\nBatch: 90 train Loss: 0.09038212895393372 train_r: [0.594249747103444, 0.15011482419593586]\nBatch: 100 train Loss: 0.06157851964235306 train_r: [0.39017303639315426, 0.0522190348911834]\nBatch: 110 train Loss: 0.05377516895532608 train_r: [0.568525685927316, 0.24674185499852508]\nBatch: 120 train Loss: 0.06427718698978424 train_r: [0.7342075840351543, 0.16406798749112275]\nBatch: 0 val Loss: 0.10934217274188995 val_r: [0.29197532715161956, 0.23735121538627812]\nBatch: 10 val Loss: 0.06583839654922485 val_r: [0.1820591726223425, 0.010578125263407862]\n{'training loss': 0.07829382232139034, 'validation loss': 0.08094861265271902, 'train r': tensor([0.4930, 0.3118]), 'val r': tensor([0.3685, 0.2127])}\n======== Epoch 7 / 10 ========\nBatch: 0 train Loss: 0.08570973575115204 train_r: [0.46474985250553114, 0.32216077022390155]\nBatch: 10 train Loss: 0.0666705071926117 train_r: [0.6666956881024773, 0.4947159098583989]\nBatch: 20 train Loss: 0.07784704864025116 train_r: [0.5707985971420955, 0.2733971596562013]\nBatch: 30 train Loss: 0.0666428655385971 train_r: [0.4365244400695596, 0.2049757349202222]\nBatch: 40 train Loss: 0.07092907279729843 train_r: [0.44111116436741066, 0.5017624517352876]\nBatch: 50 train Loss: 0.08678910881280899 train_r: [0.3791302783817614, 0.5652751270683019]\nBatch: 60 train Loss: 0.08591058105230331 train_r: [0.4350179954739475, 0.10759427614231315]\nBatch: 70 train Loss: 0.09668973088264465 train_r: [0.4381289723540073, 0.4731362965125892]\nBatch: 80 train Loss: 0.10100957006216049 train_r: [0.4858508052093929, 0.39851859357039743]\nBatch: 90 train Loss: 0.08427439630031586 train_r: [0.35825226449846176, 0.34019070016352926]\nBatch: 100 train Loss: 0.06049328297376633 train_r: [0.5189620091714521, -0.023276064381123916]\nBatch: 110 train Loss: 0.08936908096075058 train_r: [0.46818701687539066, 0.3161507084894919]\nBatch: 120 train Loss: 0.0823606327176094 train_r: [0.3611122662927603, 0.5848528771816874]\nBatch: 0 val Loss: 0.10785248875617981 val_r: [0.2942964444022879, 0.23393414192755937]\nBatch: 10 val Loss: 0.06334971636533737 val_r: [0.19868804827364414, 0.0032290882492244447]\n{'training loss': 0.0777189058384725, 'validation loss': 0.07964828261174262, 'train r': tensor([0.5076, 0.3105]), 'val r': tensor([0.3772, 0.2095])}\n======== Epoch 8 / 10 ========\nBatch: 0 train Loss: 0.05889269709587097 train_r: [0.4981193169411141, 0.08495823574352714]\nBatch: 10 train Loss: 0.05704493820667267 train_r: [0.6355114324658204, 0.3336780575919165]\nBatch: 20 train Loss: 0.09071166813373566 train_r: [0.5771628996168648, 0.3015592655409599]\nBatch: 30 train Loss: 0.06599238514900208 train_r: [0.45051179135084063, 0.3404576545930469]\nBatch: 40 train Loss: 0.053196683526039124 train_r: [0.4688231288839061, 0.008652262104792804]\nBatch: 50 train Loss: 0.06434530019760132 train_r: [0.47018163021318315, 0.4254430818225503]\nBatch: 60 train Loss: 0.0971035435795784 train_r: [0.7058345368709786, 0.32216633160745867]\nBatch: 70 train Loss: 0.09088625013828278 train_r: [0.6038809603631579, 0.4225338121176005]\nBatch: 80 train Loss: 0.10623487830162048 train_r: [0.3753801988871892, 0.21324802671645615]\nBatch: 90 train Loss: 0.07739705592393875 train_r: [0.4847489136350277, 0.554323855939162]\nBatch: 100 train Loss: 0.10316061228513718 train_r: [0.3603917583745101, 0.5248372814797141]\nBatch: 110 train Loss: 0.07498382031917572 train_r: [0.5658950337528312, -0.04503394188196763]\nBatch: 120 train Loss: 0.06831536442041397 train_r: [0.5912301521048107, 0.37462568257934326]\nBatch: 0 val Loss: 0.10885465890169144 val_r: [0.2957728263075746, 0.23156840311576818]\nBatch: 10 val Loss: 0.062223516404628754 val_r: [0.22373960813869184, 0.0005174617348958077]\n{'training loss': 0.07722594248988325, 'validation loss': 0.07931307354010642, 'train r': tensor([0.5077, 0.3146]), 'val r': tensor([0.3914, 0.2077])}\n======== Epoch 9 / 10 ========\nBatch: 0 train Loss: 0.10271459817886353 train_r: [0.460852684025664, 0.45167733301697777]\nBatch: 10 train Loss: 0.07138586044311523 train_r: [0.4957333556287421, -0.023407629538452178]\nBatch: 20 train Loss: 0.06958575546741486 train_r: [0.4638144457309684, 0.4973065922059584]\nBatch: 30 train Loss: 0.08560497313737869 train_r: [0.4328669917360103, 0.4834582093233937]\nBatch: 40 train Loss: 0.08389659225940704 train_r: [0.5895535220340657, 0.247840145851338]\nBatch: 50 train Loss: 0.07959829270839691 train_r: [0.5210931791414866, 0.36023372054291186]\nBatch: 60 train Loss: 0.06583017110824585 train_r: [0.48774541731074567, 0.509851945496971]\nBatch: 70 train Loss: 0.049592021852731705 train_r: [0.4102454167699381, 0.04153726706932198]\nBatch: 80 train Loss: 0.08064872026443481 train_r: [0.7034386246462695, 0.30490266238788644]\nBatch: 90 train Loss: 0.07114671915769577 train_r: [0.4052569751193562, 0.33102498611299125]\nBatch: 100 train Loss: 0.08209855854511261 train_r: [0.5480445507919001, 0.340459421343574]\nBatch: 110 train Loss: 0.075498066842556 train_r: [0.4886974343907703, 0.1303773723727782]\nBatch: 120 train Loss: 0.06894561648368835 train_r: [0.49416477496234246, 0.25186636927734424]\nBatch: 0 val Loss: 0.10760138928890228 val_r: [0.2969242797192783, 0.23244368271661073]\nBatch: 10 val Loss: 0.06299255788326263 val_r: [0.20890702817588339, 0.0004159734165135962]\n{'training loss': 0.07705544018083149, 'validation loss': 0.07873055804520845, 'train r': tensor([0.5087, 0.3146]), 'val r': tensor([0.3958, 0.2115])}\n======== Epoch 10 / 10 ========\nBatch: 0 train Loss: 0.12148000299930573 train_r: [0.5942965822806241, 0.6174303373597344]\nBatch: 10 train Loss: 0.06373827159404755 train_r: [0.5064899562712575, 0.24103436310977072]\nBatch: 20 train Loss: 0.055237315595149994 train_r: [0.4854332538693712, 0.2168394069253353]\nBatch: 30 train Loss: 0.0792098417878151 train_r: [0.32567866926052796, 0.07300658694621051]\nBatch: 40 train Loss: 0.06100614368915558 train_r: [0.5268316488619669, 0.2229273694484284]\nBatch: 50 train Loss: 0.06734906136989594 train_r: [0.5206364389106304, 0.5363007174746356]\nBatch: 60 train Loss: 0.06604547053575516 train_r: [0.3652940822994424, 0.34094674324796614]\nBatch: 70 train Loss: 0.07412432134151459 train_r: [0.6702025391046715, 0.3484383402783551]\nBatch: 80 train Loss: 0.04912387207150459 train_r: [0.45169417665388784, 0.1859648976360747]\nBatch: 90 train Loss: 0.0758882686495781 train_r: [0.3916906531630283, 0.49861142989357954]\nBatch: 100 train Loss: 0.07154059410095215 train_r: [0.5105613275725347, -0.015533942114422314]\nBatch: 110 train Loss: 0.05551394820213318 train_r: [0.6154774090813604, 0.25788885085191765]\nBatch: 120 train Loss: 0.08388599753379822 train_r: [0.7371191328579378, 0.1623549125941579]\nBatch: 0 val Loss: 0.10912399739027023 val_r: [0.27611915845598345, 0.2331025800792141]\nBatch: 10 val Loss: 0.06162586063146591 val_r: [0.20253236114698553, -0.0007567055624882624]\n{'training loss': 0.07645971005752919, 'validation loss': 0.07909123762510717, 'train r': tensor([0.5147, 0.3270]), 'val r': tensor([0.3951, 0.2155])}\n"
    }
   ],
   "source": [
    "# train = Train()\n",
    "train.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = {\n",
    "        \"attention_1\":train.model.attention_1.state_dict(),\n",
    "        \"attention_2\":train.model.attention_2.state_dict(),\n",
    "        \"attention_s\":train.model.attention_s.state_dict(),\n",
    "        \"features\":train.model.features.state_dict(),   \n",
    "        \"regression_1\":train.model.regression_1.state_dict(),\n",
    "        \"regression_2\":train.model.regression_1.state_dict(),\n",
    "        \"discriminator\":train.model.discriminator.state_dict()\n",
    "        }\n",
    "torch.save(model_state_dict, \"VA_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Batch: 0 val Loss: 0.07396862655878067 val_r: [0.47933176870423644, 0.3004015398340143]\n"
    }
   ],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=10000\n",
    "    )\n",
    "val_loss = []\n",
    "val_r = []\n",
    "mse = nn.MSELoss()\n",
    "training_stats = []\n",
    "for i, batch in enumerate(test_dataloader) :\n",
    "    with torch.no_grad() :\n",
    "        output, p1, p2, target = train.run_model(batch)\n",
    "\n",
    "        reg_loss = mse(output, target)\n",
    "        val_loss.append(reg_loss.item())\n",
    "        val_r.append(train.get_r(output, target))\n",
    "        if i%10 == 0 :\n",
    "            print(\"Batch: {} val Loss: {} val_r: {}\".format(i, reg_loss, val_r[-1]))\n",
    "\n",
    "# training_stats.append({\n",
    "#     'validation loss' : sum(val_loss)/len(val_loss),\n",
    "#     'val r' : torch.tensor(val_r).mean(dim=0),\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(value1, value2, label, index=None) :\n",
    "    sns.set(style='darkgrid')\n",
    "    sns.set(font_scale=1.5)\n",
    "    # plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "    if index is not None: \n",
    "        value1 = [i[index] for i in value1]\n",
    "        value2 = [i[index] for i in value2]\n",
    "    plt.plot(value1, 'b-o', label=\"Training\")\n",
    "    plt.plot(value2, 'g-o', label=\"Validation\")\n",
    "\n",
    "    plt.title(\"Training & Validation \"+format(label))\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(label)\n",
    "    plt.legend()\n",
    "    # plt.xticks(len(value1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9fbbd1aad775>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"training loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train r\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val r\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Valence R Value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df_stats = pd.DataFrame(data=train.training_stats)\n",
    "plot(df_stats[\"training loss\"].values[10:],df_stats[\"validation loss\"].values[10:],\"Loss\")\n",
    "plot(df_stats[\"train r\"],df_stats[\"val r\"],\"Valence R Value\", index=0)\n",
    "plot(df_stats[\"train r\"],df_stats[\"val r\"],\"Dominance R Value\", index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('humanoid': conda)",
   "language": "python",
   "name": "python37764bithumanoidcondac78ab0d25f4c4a4593af82371244ab3e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}